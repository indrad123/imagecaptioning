{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (0.0.43)\n",
      "Requirement already satisfied: regex in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sacremoses) (2022.7.9)\n",
      "Requirement already satisfied: six in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sacremoses) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: filelock in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/indradewaji/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('mps' if \n",
    "                      torch.backends.mps.is_available() else 'cpu')\n",
    "                      #torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Hello, how are you?\n",
      "Indonesian Translation: Halo, apa kabar?\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "# Load the model and tokenizer outside the translation function\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-id\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "def translate(text):\n",
    "# Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate translation\n",
    "    translation_ids = model.generate(input_ids, max_length=100)\n",
    "\n",
    "    # Decode the translation\n",
    "    translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return translation\n",
    "\n",
    "# Example usage\n",
    "english_text = \"Hello, how are you?\"\n",
    "indonesian_translation = translate(english_text)\n",
    "\n",
    "print(f\"English: {english_text}\")\n",
    "print(f\"Indonesian Translation: {indonesian_translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('captions.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translate = pandas.DataFrame(columns=['image', 'caption_en', 'caption_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 29min 46s, sys: 1h 45min 6s, total: 6h 14min 52s\n",
      "Wall time: 1d 1h 52min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_translate['image'], df_translate['caption_en'], df_translate['caption_id'] = df['image'], df['caption'], df['caption'].apply(translate)\n",
    "df_translate.to_csv('captions_id.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# Assuming you have downloaded the Flickr8k dataset and have the images and captions\n",
    "\n",
    "# Sample data loading and preprocessing\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, image_folder, captions_file, transform=None):\n",
    "        # Load image paths and captions\n",
    "        self.image_folder = image_folder\n",
    "        self.captions = self.load_captions(captions_file)\n",
    "        self.image_paths = list(self.captions.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_captions(self, captions_file):\n",
    "        # Load captions from the file\n",
    "        captions = {}\n",
    "        with open(captions_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split('\\t')\n",
    "                img_name, caption = parts[0], parts[1]\n",
    "                img_name = img_name.split(\"#\")[0]  # Remove the #n from image names\n",
    "                if img_name not in captions:\n",
    "                    captions[img_name] = []\n",
    "                captions[img_name].append(caption)\n",
    "        return captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        img_path = f'{self.image_folder}/{img_name}'\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image and corresponding captions\n",
    "        return image, self.captions[img_name]\n",
    "\n",
    "# Define the image captioning model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        # CNN for image feature extraction\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_size)\n",
    "\n",
    "        # RNN for caption generation\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        # Image feature extraction\n",
    "        image_features = self.cnn(images)\n",
    "\n",
    "        # Embedding captions\n",
    "        captions = self.embedding(captions)\n",
    "\n",
    "        # Concatenate image features and captions\n",
    "        inputs = torch.cat((image_features.unsqueeze(1), captions), 1)\n",
    "\n",
    "        # Pack sequences for LSTM\n",
    "        packed_inputs = pack_padded_sequence(inputs, lengths, batch_first=True)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        packed_outputs, _ = self.rnn(packed_inputs)\n",
    "\n",
    "        # Fully connected layer\n",
    "        outputs = self.fc(packed_outputs.data)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = 10000  # Adjust based on your dataset vocabulary size\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = Flickr8kDataset(image_folder='path/to/your/images', captions_file='path/to/your/captions.txt', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, captions_list in dataloader:\n",
    "        # Concatenate all captions into one list\n",
    "        captions = [caption for captions_per_image in captions_list for caption in captions_per_image]\n",
    "\n",
    "        # Tokenize captions\n",
    "        captions = [word_tokenize(caption.lower()) for caption in captions]\n",
    "\n",
    "        # Convert captions to indices using a vocabulary\n",
    "        # (Assuming you have a vocabulary with word-to-index mapping)\n",
    "        captions = [[vocab[word] for word in caption] for caption in captions]\n",
    "\n",
    "        # Find the maximum length of captions\n",
    "        max_len = max(len(caption) for caption in captions)\n",
    "\n",
    "        # Pad captions to the maximum length\n",
    "        captions_padded = [caption + [0] * (max_len - len(caption)) for caption in captions]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        captions_tensor = torch.tensor(captions_padded)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, captions_tensor, [max_len] * batch_size)\n",
    "\n",
    "        # Calculate the loss\n",
    "        targets = captions_tensor[:, 1:].contiguous().view(-1)  # Exclude the <start> token\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
