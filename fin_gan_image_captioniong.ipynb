{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Bkbbjb-KLqJRSY9Z1HUBenm5E6AGtYxJ",
      "authorship_tag": "ABX9TyPL1saIBzMXHx6GJu+5odsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indrad123/imagecaptioning/blob/main/fin_gan_image_captioniong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas  pillow nltk\n",
        "!pip install datasets\n",
        "!pip install --upgrade lxml\n",
        "!pip install openimages\n",
        "!pip install -U torchtext==0.6\n",
        "!pip install pycocotools\n",
        "!pip install torch_summary\n",
        "!pip install nltk\n",
        "!pip install torchvision\n",
        "!pip install rouge_score\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO0168Lu1bhu",
        "outputId": "64b7da54-cbd3-4eb1-ac40-870059e87fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (5.2.2)\n",
            "Requirement already satisfied: openimages in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from openimages) (1.34.122)\n",
            "Requirement already satisfied: cvdata in /usr/local/lib/python3.10/dist-packages (from openimages) (0.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from openimages) (5.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openimages) (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openimages) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openimages) (4.66.4)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.122 in /usr/local/lib/python3.10/dist-packages (from boto3->openimages) (1.34.122)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->openimages) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->openimages) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cvdata->openimages) (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from cvdata->openimages) (4.8.0.76)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cvdata->openimages) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openimages) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openimages) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openimages) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openimages) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openimages) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openimages) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openimages) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openimages) (1.16.0)\n",
            "Requirement already satisfied: torchtext==0.6 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Requirement already satisfied: torch_summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=657b0773f32d9e67fa3a2a36af2d20507aad751f368c8a47c2a108e3f1f2e921\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.31.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-Train Approach"
      ],
      "metadata": {
        "id": "-by6V8TL9S3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import io\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download NLTK data if not present\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"indrad123/flickr30k-google-translate-id\")\n",
        "\n",
        "# Prepare the dataset\n",
        "data = pd.DataFrame({\n",
        "    \"image\": dataset[\"test\"][\"image\"],\n",
        "    \"caption\": dataset[\"test\"][\"original_alt_text_id\"]\n",
        "})\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = train_test_split(data, test_size=0.05, random_state=42)\n",
        "\n",
        "# Define the custom dataset\n",
        "class CustomImageCaptionDataset(Dataset):\n",
        "    def __init__(self, data, img_transform=None):\n",
        "        self.data = data\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_bytes = self.data.iloc[idx]['image']['bytes']\n",
        "        caption = self.data.iloc[idx]['caption']\n",
        "\n",
        "        image = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        return image, caption\n",
        "\n",
        "# Define image transformations\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = CustomImageCaptionDataset(train_data, img_transform)\n",
        "val_dataset = CustomImageCaptionDataset(val_data, img_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Define the Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, image_encoder_model, text_decoder_model, feature_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decoder_model)\n",
        "        self.encoder.encoder.pooler_output = nn.Linear(self.encoder.encoder.config.hidden_size, feature_dim)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(text_decoder_model)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        vocab_size = self.tokenizer.vocab_size\n",
        "        hidden_dim = self.encoder.decoder.config.hidden_size\n",
        "\n",
        "        self.decoder = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder.encoder(images).pooler_output\n",
        "        captions_encoded = self.tokenizer(captions, return_tensors='pt', padding=True, truncation=True)\n",
        "        embeddings = self.embed(captions_encoded['input_ids'].cuda())\n",
        "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        hiddens, _ = self.decoder(inputs)\n",
        "        outputs = self.fc(hiddens)\n",
        "        generated_ids = torch.argmax(outputs, dim=-1)\n",
        "        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Define the Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * 28 * 28, 1)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        x = torch.relu(self.conv1(images))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = torch.sigmoid(self.fc1(x))\n",
        "        return outputs\n",
        "\n",
        "# Hyperparameters\n",
        "feature_dim = 768\n",
        "\n",
        "# Move models to GPU\n",
        "generator = Generator(\"google/vit-base-patch16-224-in21k\", \"gpt2\", feature_dim).cuda()\n",
        "discriminator = Discriminator().cuda()\n",
        "\n",
        "# Define optimizer and loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with mixed precision\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    for images, captions in train_loader:\n",
        "        images = images.cuda()\n",
        "        captions = [str(caption) for caption in captions]\n",
        "\n",
        "        real_labels = torch.ones(images.size(0), 1).cuda()\n",
        "        fake_labels = torch.zeros(images.size(0), 1).cuda()\n",
        "\n",
        "        # Check tensor locations\n",
        "        print(f'Image device: {images.device}')\n",
        "        print(f'Real labels device: {real_labels.device}')\n",
        "        print(f'Fake labels device: {fake_labels.device}')\n",
        "\n",
        "        # Discriminator training\n",
        "        discriminator.zero_grad()\n",
        "        with autocast():\n",
        "            real_outputs = discriminator(images, captions)\n",
        "            d_loss_real = criterion(real_outputs, real_labels)\n",
        "\n",
        "        scaler.scale(d_loss_real).backward()\n",
        "\n",
        "        fake_captions = generator(images, captions)\n",
        "        with autocast():\n",
        "            fake_outputs = discriminator(images, fake_captions)\n",
        "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        scaler.scale(d_loss_fake).backward()\n",
        "        scaler.step(d_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Generator training\n",
        "        generator.zero_grad()\n",
        "        fake_captions = generator(images, captions)\n",
        "        with autocast():\n",
        "            fake_outputs = discriminator(images, fake_captions)\n",
        "            g_loss = criterion(fake_outputs, real_labels)\n",
        "\n",
        "        scaler.scale(g_loss).backward()\n",
        "\n",
        "        # Ensure there are gradients to update\n",
        "        for name, param in generator.named_parameters():\n",
        "            if param.grad is None:\n",
        "                print(f\"No gradients for {name}\")\n",
        "\n",
        "        # Check if any parameter has gradients before step\n",
        "        has_gradients = any(param.grad is not None for param in generator.parameters())\n",
        "        if not has_gradients:\n",
        "            print(\"No gradients computed for generator parameters. Skipping optimizer step.\")\n",
        "            continue\n",
        "\n",
        "        scaler.step(g_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss_real.item() + d_loss_fake.item()}, g_loss: {g_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pgjWfGv9b1S",
        "outputId": "6fe83ecc-2c9f-4b12-f520-8c5d7c43142c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Image device: cuda:0\n",
            "Real labels device: cuda:0\n",
            "Fake labels device: cuda:0\n",
            "No gradients for encoder.encoder.embeddings.cls_token\n",
            "No gradients for encoder.encoder.embeddings.position_embeddings\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.weight\n",
            "No gradients for encoder.encoder.embeddings.patch_embeddings.projection.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.0.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.1.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.2.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.3.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.4.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.5.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.6.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.7.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.8.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.9.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.10.layernorm_after.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.query.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.key.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.attention.value.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.attention.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.intermediate.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.output.dense.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_before.bias\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.weight\n",
            "No gradients for encoder.encoder.encoder.layer.11.layernorm_after.bias\n",
            "No gradients for encoder.encoder.layernorm.weight\n",
            "No gradients for encoder.encoder.layernorm.bias\n",
            "No gradients for encoder.encoder.pooler.dense.weight\n",
            "No gradients for encoder.encoder.pooler.dense.bias\n",
            "No gradients for encoder.encoder.pooler_output.weight\n",
            "No gradients for encoder.encoder.pooler_output.bias\n",
            "No gradients for encoder.decoder.transformer.wte.weight\n",
            "No gradients for encoder.decoder.transformer.wpe.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.0.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.1.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.2.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.3.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.4.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.5.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.6.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.7.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.8.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.9.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.10.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_1.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.attn.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_2.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.q_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.crossattention.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.ln_cross_attn.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_fc.bias\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.weight\n",
            "No gradients for encoder.decoder.transformer.h.11.mlp.c_proj.bias\n",
            "No gradients for encoder.decoder.transformer.ln_f.weight\n",
            "No gradients for encoder.decoder.transformer.ln_f.bias\n",
            "No gradients for decoder.weight_ih_l0\n",
            "No gradients for decoder.weight_hh_l0\n",
            "No gradients for decoder.bias_ih_l0\n",
            "No gradients for decoder.bias_hh_l0\n",
            "No gradients for embed.weight\n",
            "No gradients for fc.weight\n",
            "No gradients for fc.bias\n",
            "No gradients computed for generator parameters. Skipping optimizer step.\n",
            "Epoch [10/10], d_loss: 1.3862943649291992, g_loss: 0.6931471824645996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    pixel_values = torch.stack(images)  # Stack images to form a batch\n",
        "\n",
        "    # Check if captions are valid and not empty\n",
        "    valid_captions = []\n",
        "    for caption in captions:\n",
        "\n",
        "        if isinstance(caption, list) and len(caption) > 0:\n",
        "            valid_captions.append(caption[0])  # If list, take the first caption\n",
        "        elif isinstance(caption, str):\n",
        "            valid_captions.append(caption)     # If string, take the whole caption\n",
        "        else:\n",
        "            valid_captions.append(\"\")          # Handle other invalid cases\n",
        "\n",
        "    captions_encoded = tokenizer(valid_captions, padding='max_length', max_length=64, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Prepare labels\n",
        "    labels = captions_encoded.input_ids\n",
        "    labels = torch.where(labels != tokenizer.pad_token_id, labels, -100)\n",
        "\n",
        "    # Create attention masks\n",
        "    decoder_attention_mask = captions_encoded.attention_mask\n",
        "\n",
        "    # Debugging: Print shapes to verify dimensions\n",
        "    print(\"pixel_values shape:\", pixel_values.shape)\n",
        "    print(\"input_ids shape:\", captions_encoded.input_ids.shape)\n",
        "    print(\"labels shape:\", labels.shape)\n",
        "    print(\"decoder_attention_mask shape:\", decoder_attention_mask.shape)\n",
        "    print(\"Sample valid captions:\", valid_captions[:5])\n",
        "\n",
        "    # Return inputs for the model\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'labels': labels,\n",
        "        'decoder_input_ids': captions_encoded.input_ids,  # Add decoder_input_ids\n",
        "        'decoder_attention_mask': decoder_attention_mask,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "yZ5G14cc9Sb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the generator and discriminator models\n",
        "generator_path = \"/content/drive/MyDrive/GANModelImageCaptioning/model/generator.pth\"\n",
        "discriminator_path = \"/content/drive/MyDrive/GANModelImageCaptioning/model/discriminator.pth\"\n",
        "\n",
        "torch.save(generator.state_dict(), generator_path)\n",
        "torch.save(discriminator.state_dict(), discriminator_path)\n",
        "\n",
        "print(\"Models saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx_uqNkq9i8P",
        "outputId": "1c593ed8-aa2e-4988-e93d-2113233d55c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ov-z1iK9Q8l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the generator and discriminator models\n",
        "image_encoder_model = \"google/vit-base-patch16-224-in21k\"  # Define the image encoder model\n",
        "text_decoder_model = \"gpt2\"  # Define the text decoder model\n",
        "feature_dim = 768  # Define the feature dimension\n",
        "\n",
        "loaded_generator = Generator(image_encoder_model, text_decoder_model, feature_dim)  # Pass the necessary arguments\n",
        "loaded_discriminator = Discriminator()  # No need for feature_dim and hidden_dim in Discriminator\n",
        "\n",
        "# Load the state dictionaries\n",
        "loaded_generator.load_state_dict(torch.load(generator_path))\n",
        "loaded_discriminator.load_state_dict(torch.load(discriminator_path))\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "loaded_generator.eval()\n",
        "loaded_discriminator.eval()\n",
        "\n",
        "print(\"Models loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUC14GLa9qxL",
        "outputId": "817f9b3c-6f31-407c-eb18-9198533a860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Assuming you used a GPT-2 tokenizer before\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def generate_caption(generator, image_path, tokenizer, max_length=64):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = img_transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        features = generator.encoder.encoder(image)\n",
        "        # Extract the 'last_hidden_state' from the features\n",
        "        features = features.last_hidden_state\n",
        "        input_ids = torch.tensor(tokenizer.encode(tokenizer.bos_token)).unsqueeze(0)\n",
        "        # Initialize the hidden state for the LSTM\n",
        "        hidden_state = None\n",
        "        for _ in range(max_length):\n",
        "            # Pass the image features as the initial hidden state if it's the first step\n",
        "            if hidden_state is None:\n",
        "                # Remove 'input_ids' and pass features as input\n",
        "                outputs, hidden_state = generator.decoder(features)\n",
        "            else:\n",
        "                # Pass the hidden state as input to the LSTM\n",
        "                outputs, hidden_state = generator.decoder(hidden_state[0])\n",
        "            # Extract the last hidden state from the decoder output\n",
        "            last_hidden_state = outputs  # Assuming outputs is the hidden state\n",
        "            predicted_id = torch.argmax(last_hidden_state[0, -1, :]).item()\n",
        "            if predicted_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[predicted_id]])], dim=-1)\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Generate caption for a new image\n",
        "image_path = \"/content/drive/MyDrive/Pictures/KopitemaCUps.JPG\"\n",
        "caption = generate_caption(loaded_generator, image_path, tokenizer)\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgUA64949reR",
        "outputId": "06359a5b-d750-432a-dc32-bac447116f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: oll+iousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousiousious\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Hugging Face"
      ],
      "metadata": {
        "id": "hIRHkAlE97tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "# Save final model\n",
        "output_dir = \"/content/drive/MyDrive/VitImageCaptioning/model\"\n",
        "generator.encoder.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "6qIV12eA993r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Define your model repository name\n",
        "model_repo_name = \"your-username/vit-gpt2-image-captioning\"\n",
        "\n",
        "# Save final model and tokenizer\n",
        "output_dir = \"/content/drive/MyDrive/VitImageCaptioning/model\"\n",
        "generator.encoder.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Push the model to Hugging Face Hub\n",
        "model = VisionEncoderDecoderModel.from_pretrained(output_dir)\n",
        "model.push_to_hub(model_repo_name)\n",
        "\n",
        "# Push tokenizer to Hugging Face Hub\n",
        "tokenizer.push_to_hub(model_repo_name)\n"
      ],
      "metadata": {
        "id": "VihsqJeJ9-v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "AoOuZ7QaKEbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets evaluate googletrans==4.0.0-rc1 nltk matplotlib\n"
      ],
      "metadata": {
        "id": "qa4bk0ke-BTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1611de9f-18b5-42b6-b376-5b932d2bf747"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=76809ee4831d082f980e612b9ff354fd92a2abc8a72766a264e0befa20196e0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, hstspreload, h2, dill, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, nvidia-cusolver-cu12, httpx, googletrans, datasets, evaluate\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 datasets-2.19.2 dill-0.3.8 evaluate-0.4.2 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 requests-2.32.3 rfc3986-1.5.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
        "from googletrans import Translator\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Ensure the NLTK punkt tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, image_encoder_model, text_decoder_model, feature_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decoder_model)\n",
        "        self.encoder.encoder.pooler_output = nn.Linear(self.encoder.encoder.config.hidden_size, feature_dim)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(text_decoder_model)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        vocab_size = self.tokenizer.vocab_size\n",
        "        hidden_dim = self.encoder.decoder.config.hidden_size\n",
        "\n",
        "        self.decoder = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder.encoder(images).pooler_output\n",
        "        captions_encoded = self.tokenizer(captions, return_tensors='pt', padding=True, truncation=True)\n",
        "        # In the forward method of the Generator class\n",
        "        embeddings = self.embed(captions_encoded['input_ids'].long().to(device)) # Explicitly cast to LongTensor # Move to device if necessary, but keep LongTensor type\n",
        "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        hiddens, _ = self.decoder(inputs)\n",
        "        outputs = self.fc(hiddens)\n",
        "        generated_ids = torch.argmax(outputs, dim=-1)\n",
        "        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "# Define the Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * 28 * 28, 1)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        x = torch.relu(self.conv1(images))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = torch.sigmoid(self.fc1(x))\n",
        "        return outputs\n",
        "\n",
        "# Hyperparameters\n",
        "feature_dim = 768\n",
        "\n",
        "# Move models to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator(\"google/vit-base-patch16-224-in21k\", \"gpt2\", feature_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6b30KhsLF8O",
        "outputId": "0cf34c8f-f02a-49c9-dc20-ca476b8168c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"indrad123/flickr30k-google-translate-id\")\n",
        "\n",
        "# Prepare the dataset and select 100 random samples\n",
        "data = pd.DataFrame({\n",
        "    \"image\": dataset[\"test\"][\"image\"],\n",
        "    \"caption\": dataset[\"test\"][\"alt_text_id\"]\n",
        "}).sample(n=10000, random_state=42)\n",
        "\n",
        "# Define image transformations\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomImageCaptionDataset(Dataset):\n",
        "    def __init__(self, data, img_transform=None):\n",
        "        self.data = data\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_bytes = self.data.iloc[idx]['image']['bytes']\n",
        "        caption = self.data.iloc[idx]['caption']\n",
        "        image = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        return image, caption\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = CustomImageCaptionDataset(data, img_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZUWIXvGLh5f",
        "outputId": "670782c9-54e4-423d-e1e3-ad527ea2ae63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvubbJ0aMpNA",
        "outputId": "e3e21c28-0dc9-4780-f444-3192d147e29b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=cf0af3c1e404d6c70d9debcc81059640ed770daebf5324273f1599ff51095c38\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "# Load evaluation metrics\n",
        "bleu_metric = evaluate.load('bleu')\n",
        "rouge_metric = evaluate.load('rouge')\n",
        "meteor_metric = evaluate.load('meteor')\n",
        "\n",
        "# Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate text\n",
        "def translate_text(text, src_lang='en', tgt_lang='id'):\n",
        "    try:\n",
        "        translated = translator.translate(text, src=src_lang, dest=tgt_lang).text\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Generate captions and evaluate\n",
        "generated_captions = []\n",
        "reference_captions = []\n",
        "\n",
        "for images, captions in tqdm(dataloader, desc=\"Generating Captions\"):\n",
        "    images = images.to(device)\n",
        "    reference_caption = captions[0]\n",
        "\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        generated_caption_en = generator(images, [\"\"])[0]  # Empty caption to trigger generation\n",
        "        generated_caption_id = translate_text(generated_caption_en)\n",
        "\n",
        "    generated_captions.append(generated_caption_id)\n",
        "    reference_captions.append(reference_caption)\n",
        "\n",
        "# Post-process text\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "generated_captions, reference_captions = postprocess_text(generated_captions, reference_captions)\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_score = bleu_metric.compute(predictions=generated_captions, references=[[ref] for ref in reference_captions])['bleu']\n",
        "\n",
        "# Compute ROUGE score\n",
        "rouge_score = rouge_metric.compute(predictions=generated_captions, references=reference_captions)\n",
        "rouge_l_f1 = rouge_score['rougeL'] if isinstance(rouge_score['rougeL'], float) else rouge_score['rougeL'].mid.fmeasure\n",
        "\n",
        "# Compute METEOR score\n",
        "meteor_score = meteor_metric.compute(predictions=generated_captions, references=reference_captions)['meteor']\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"GAN\"],\n",
        "    \"BLEU\": [bleu_score],\n",
        "    \"METEOR\": [meteor_score],\n",
        "    \"ROUGE\": [rouge_l_f1]\n",
        "})\n",
        "\n",
        "# Print the results\n",
        "print(results_df)\n",
        "\n",
        "# Plot the evaluation metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "bar_width = 0.2\n",
        "r1 = np.arange(len(results_df))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + bar_width for x in r2]\n",
        "\n",
        "plt.bar(r1, results_df['BLEU'], color='blue', width=bar_width, edgecolor='grey', label='BLEU')\n",
        "plt.bar(r2, results_df['METEOR'], color='green', width=bar_width, edgecolor='grey', label='METEOR')\n",
        "plt.bar(r3, results_df['ROUGE'], color='red', width=bar_width, edgecolor='grey', label='ROUGE')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Scores for Image Captioning Model')\n",
        "plt.xticks([r + bar_width for r in range(len(results_df))], results_df['Model'])\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "QygVjhX0MhEF",
        "outputId": "246fde48-f5a7-4246-aa5a-1fbf33e0ee09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Generating Captions:   0%|          | 0/10000 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Generating Captions: 100%|██████████| 10000/10000 [44:19<00:00,  3.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model  BLEU    METEOR     ROUGE\n",
            "0   GAN   0.0  0.011925  0.000825\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5 random images from the dataset\n",
        "sampled_data = data.sample(n=5, random_state=42)\n",
        "\n",
        "# Initialize the generator model and tokenizer\n",
        "feature_dim = 768\n",
        "generator = Generator(\"google/vit-base-patch16-224-in21k\", \"gpt2\", feature_dim).cuda()\n",
        "generator.eval()  # Set the generator to evaluation mode\n",
        "\n",
        "# Define a function to preprocess images\n",
        "def preprocess_image(image_base64, transform):\n",
        "    image_bytes = base64.b64decode(image_base64)\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "    return transform(image)\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Loop through the sampled data and generate captions\n",
        "for index, row in sampled_data.iterrows():\n",
        "    image_base64 = row['image']['bytes']\n",
        "    reference_caption = row['caption']\n",
        "\n",
        "    image_tensor = preprocess_image(image_base64, transform).unsqueeze(0).cuda()  # Add batch dimension and move to GPU\n",
        "    with torch.no_grad():\n",
        "        generated_caption = generator(image_tensor, [reference_caption])[0]\n",
        "\n",
        "    # Display the image, generated caption, and reference caption\n",
        "    image = Image.open(io.BytesIO(base64.b64decode(image_base64)))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Generated: {generated_caption}\\nReference: {reference_caption}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pEKeMDzpU97_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}