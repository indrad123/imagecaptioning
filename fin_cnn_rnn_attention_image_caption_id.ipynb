{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1tS1aMQwMkMvIvxZjgolmmTK9oBiG8WqB",
      "authorship_tag": "ABX9TyMox+YC/YoC0kcLbTZkpebc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indrad123/imagecaptioning/blob/main/fin_cnn_rnn_attention_image_caption_id.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "dHWCMxfMyYje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yREQpkj6EXMt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtext.data import Field\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import models\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.model_selection import train_test_split\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Device configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define the checkpoint path\n",
        "checkpoint_dir = '/content/drive/MyDrive/CnnRnnAttention/checkpoint'\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth.tar')\n",
        "\n",
        "# Define the model save path\n",
        "model_dir = '/content/drive/MyDrive/CnnRnnAttention/model'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Ensure the checkpoint directory exists\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"Mozilla/flickr30k-transformed-captions\")\n",
        "\n",
        "# Prepare the dataset\n",
        "data = pd.DataFrame({\n",
        "    \"image\": dataset[\"test\"][\"image\"],\n",
        "    \"caption\": dataset[\"test\"][\"original_alt_text\"]\n",
        "})\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = train_test_split(data, test_size=0.05, random_state=42)\n",
        "\n",
        "# Define captions field\n",
        "captions = Field(sequential=False, init_token='<start>', eos_token='<end>')\n",
        "\n",
        "# Build vocabulary\n",
        "all_captions = train_data['caption'].tolist()\n",
        "all_tokens = [[w.lower() for w in c.split()] for c in all_captions]\n",
        "all_tokens = [w for sublist in all_tokens for w in sublist]\n",
        "captions.build_vocab(all_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    pass\n",
        "\n",
        "vocab = Vocab()\n",
        "captions.vocab.itos.insert(0, '<pad>')\n",
        "vocab.itos = captions.vocab.itos\n",
        "vocab.stoi = defaultdict(lambda: captions.vocab.itos.index('<unk>'))\n",
        "vocab.stoi['<pad>'] = 0\n",
        "for s, i in captions.vocab.stoi.items():\n",
        "    vocab.stoi[s] = i + 1\n",
        "\n",
        "# Custom dataset class\n",
        "class CaptioningData(Dataset):\n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index].squeeze()\n",
        "        image = Image.open(row.image).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        caption = row.caption.lower().split()\n",
        "        target = [self.vocab.stoi['<start>']] + [self.vocab.stoi[token] for token in caption] + [self.vocab.stoi['<end>']]\n",
        "        target = torch.Tensor(target).long()\n",
        "        return image, target, row.caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, targets, captions = zip(*data)\n",
        "        images = torch.stack(images, 0)\n",
        "        lengths = [len(tar) for tar in targets]\n",
        "        padded_targets = torch.zeros(len(targets), max(lengths)).long()\n",
        "        for i, tar in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            padded_targets[i, :end] = tar[:end]\n",
        "        return images.to(device), padded_targets.to(device), torch.tensor(lengths).long().to(device), captions\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsnrCvSUypsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save intermediate data to H5\n",
        "def save_to_h5(dataset, file_name):\n",
        "    with h5py.File(file_name, 'w') as h:\n",
        "        images = []\n",
        "        captions = []\n",
        "        for img, cap, _ in tqdm(dataset):\n",
        "            images.append(img.numpy())\n",
        "            captions.append(cap.numpy())\n",
        "        h.create_dataset('images', data=np.array(images))\n",
        "        h.create_dataset('captions', data=np.array(captions))\n",
        "\n",
        "# Load intermediate data from H5\n",
        "def load_from_h5(file_name):\n",
        "    with h5py.File(file_name, 'r') as h:\n",
        "        images = h['images'][:]\n",
        "        captions = h['captions'][:]\n",
        "    return images, captions\n",
        "\n",
        "# Save training and validation datasets to H5\n",
        "save_to_h5(CaptioningData(train_data, vocab), 'train_data.h5')\n",
        "save_to_h5(CaptioningData(val_data, vocab), 'val_data.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "vIGM39NBy10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class for loading H5 data\n",
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, file_name):\n",
        "        self.images, self.captions = load_from_h5(file_name)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = torch.tensor(self.images[index])\n",
        "        caption = torch.tensor(self.captions[index]).long()\n",
        "        return image, caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, targets = zip(*data)\n",
        "        images = torch.stack(images, 0)\n",
        "        lengths = [len(tar) for tar in targets]\n",
        "        padded_targets = torch.zeros(len(targets), max(lengths)).long()\n",
        "        for i, tar in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            padded_targets[i, :end] = tar[:end]\n",
        "        return images.to(device), padded_targets.to(device), torch.tensor(lengths).long().to(device)\n",
        "\n",
        "# Load datasets from H5\n",
        "train_ds = H5Dataset('train_data.h5')\n",
        "val_ds = H5Dataset('val_data.h5')\n",
        "\n",
        "# Use multiple workers and prefetching\n",
        "train_dl = DataLoader(train_ds, batch_size=32, collate_fn=train_ds.collate_fn, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, collate_fn=val_ds.collate_fn, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "IOjTE2jpy_4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "encoder = EncoderCNN(256).to(device)\n",
        "\n",
        "# Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        packed = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True)\n",
        "        outputs, _ = self.lstm(packed)\n",
        "        outputs = self.linear(outputs[0])\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, features, states=None):\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))\n",
        "            _, predicted = outputs.max(1)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)\n",
        "        sentences = []\n",
        "        for sampled_id in sampled_ids:\n",
        "            sampled_id = sampled_id.cpu().numpy()\n",
        "            sampled_caption = []\n",
        "            for word_id in sampled_id:\n",
        "                word = vocab.itos[word_id]\n",
        "                sampled_caption.append(word)\n",
        "                if word == '<end>':\n",
        "                    break\n",
        "            sentence = ' '.join(sampled_caption)\n",
        "            sentences.append(sentence)\n",
        "        return sentences\n",
        "\n",
        "decoder = DecoderRNN(256, 512, len(vocab.itos), 1).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "4kYA8lCwzGuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-3)\n",
        "\n",
        "# Mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Function to save the model checkpoint\n",
        "def save_checkpoint(state, filename):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# Function to load the model checkpoint\n",
        "def load_checkpoint(filename, encoder, decoder, optimizer):\n",
        "    if os.path.isfile(filename):\n",
        "        print(f\"Loading checkpoint '{filename}'\")\n",
        "        checkpoint = torch.load(filename)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(f\"Checkpoint loaded successfully from '{filename}' at (epoch {checkpoint['epoch']})\")\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{filename}'\")\n",
        "        start_epoch = 0\n",
        "    return start_epoch\n",
        "\n",
        "# Training function\n",
        "def train_batch(data, encoder, decoder, optimizer, criterion):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    images, captions, lengths, _ = data\n",
        "    images = images.to(device)\n",
        "    captions = captions.to(device)\n",
        "    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with autocast():\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions, lengths)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_batch(data, encoder, decoder, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    images, captions, lengths, _ = data\n",
        "    images = images.to(device)\n",
        "    captions = captions.to(device)\n",
        "    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n",
        "    features = encoder(images)\n",
        "    outputs = decoder(features, captions, lengths)\n",
        "    loss = criterion(outputs, targets)\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(encoder, decoder, data_loader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    smooth = SmoothingFunction().method4\n",
        "    total_bleu_score = 0\n",
        "    total_samples = 0\n",
        "    for i, data in enumerate(data_loader):\n",
        "        images, captions, lengths, original_captions = data\n",
        "        images = images.to(device)\n",
        "        features = encoder(images)\n",
        "        predicted_captions = decoder.predict(features)\n",
        "        for pred, true in zip(predicted_captions, original_captions):\n",
        "            true_tokens = [w.lower() for w in nltk.word_tokenize(true)]\n",
        "            pred_tokens = [w for w in pred.split() if w not in ('<start>', '<end>', '<pad>')]\n",
        "            bleu_score = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth)\n",
        "            total_bleu_score += bleu_score\n",
        "            total_samples += 1\n",
        "    return total_bleu_score / total_samples\n",
        "\n"
      ],
      "metadata": {
        "id": "zhm9AFNfzQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from checkpoint if exists\n",
        "start_epoch = load_checkpoint(checkpoint_path, encoder, decoder, optimizer)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 10\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "    if epoch == 5:\n",
        "        optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(train_dl):\n",
        "        batch_loss = train_batch(data, encoder, decoder, optimizer, criterion)\n",
        "        train_loss += batch_loss\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i + 1}/{len(train_dl)}], Loss: {batch_loss:.4f}')\n",
        "\n",
        "    train_loss /= len(train_dl)\n",
        "    print(f'Epoch [{epoch + 1}/{n_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "    val_loss = 0.0\n",
        "    for i, data in enumerate(val_dl):\n",
        "        batch_loss = validate_batch(data, encoder, decoder, criterion)\n",
        "        val_loss += batch_loss\n",
        "\n",
        "    val_loss /= len(val_dl)\n",
        "    print(f'Epoch [{epoch + 1}/{n_epochs}], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Evaluate model\n",
        "    bleu_score = evaluate_model(encoder, decoder, val_dl)\n",
        "    print(f'Epoch [{epoch + 1}/{n_epochs}], BLEU Score: {bleu_score:.4f}')\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "IR0fnLdtzXtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model\n",
        "torch.save({\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': epoch\n",
        "}, os.path.join(model_dir, 'model.pth'))\n",
        "\n",
        "# Save vocab to Google Drive\n",
        "import json\n",
        "with open(os.path.join(model_dir, 'vocab.json'), 'w') as f:\n",
        "    json.dump({'itos': vocab.itos, 'stoi': dict(vocab.stoi)}, f)\n",
        "\n",
        "# Load model function for inference\n",
        "def load_model(encoder, decoder, optimizer, model_dir):\n",
        "    encoder.load_state_dict(torch.load(os.path.join(model_dir, 'encoder.pth')))\n",
        "    decoder.load_state_dict(torch.load(os.path.join(model_dir, 'decoder.pth')))\n",
        "    optimizer.load_state_dict(torch.load(os.path.join(model_dir, 'optimizer.pth')))\n",
        "    return encoder, decoder, optimizer\n",
        "\n",
        "# Prediction function\n",
        "@torch.no_grad()\n",
        "def load_image_and_predict(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    features = encoder(image)\n",
        "    sentence = decoder.predict(features)[0]\n",
        "    return sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DYeSCY7CzhEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_path = 'path_to_an_image.jpg'\n",
        "caption = load_image_and_predict(image_path)\n",
        "print('Generated Caption:', caption)"
      ],
      "metadata": {
        "id": "BjXNl_Yozi5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}