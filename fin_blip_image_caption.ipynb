{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMn/9ZT7/6uUy2Cl4imjez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indrad123/imagecaptioning/blob/main/fin_blip_image_caption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install pillow\n",
        "!pip install torchvision\n",
        "!pip install h5py\n",
        "!pip install datasets pandas scikit-learn torch transformers tqdm pillow torchvision h5py\n"
      ],
      "metadata": {
        "id": "dqFW2pnGqLQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu9u-KiCpzMI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import h5py\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"indrad123/flickr30k-transformed-captions-indonesia\")\n",
        "\n",
        "# Prepare the dataset\n",
        "data = pd.DataFrame({\n",
        "    \"image\": dataset[\"test\"][\"image\"],\n",
        "    \"caption\": dataset[\"test\"][\"original_alt_text_id\"]\n",
        "})\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = train_test_split(data, test_size=0.05, random_state=42)\n",
        "\n",
        "# Define captions field\n",
        "class Vocab:\n",
        "    pass\n",
        "\n",
        "captions = Vocab()\n",
        "captions.itos = ['<pad>', '<start>', '<end>', '<unk>']  # Initial tokens\n",
        "\n",
        "# Build vocabulary\n",
        "all_captions = train_data['caption'].tolist()\n",
        "all_tokens = [[w.lower() for w in c.split()] for c in all_captions]\n",
        "all_tokens = [w for sublist in all_tokens for w in sublist]\n",
        "captions.itos.extend(list(set(all_tokens)))\n",
        "captions.stoi = defaultdict(lambda: captions.itos.index('<unk>'), {s: i for i, s in enumerate(captions.itos)})\n",
        "\n",
        "# Custom dataset class\n",
        "class CaptioningData(Dataset):\n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index].squeeze()\n",
        "        image = Image.open(io.BytesIO(row.image['bytes'])).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        caption = row.caption.lower().split()\n",
        "        target = [self.vocab.stoi['<start>']] + [self.vocab.stoi[token] for token in caption] + [self.vocab.stoi['<end>']]\n",
        "        target = torch.Tensor(target).long()\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, targets = zip(*data)\n",
        "        images = torch.stack(images, 0)\n",
        "        lengths = [len(tar) for tar in targets]\n",
        "        padded_targets = torch.zeros(len(targets), max(lengths)).long()\n",
        "        for i, tar in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            padded_targets[i, :end] = tar[:end]\n",
        "        return images.to(device), padded_targets.to(device), torch.tensor(lengths).long().to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "yv_UojhUqmG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "train_dataset = CaptioningData(train_data, captions)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=train_dataset.collate_fn)\n",
        "\n",
        "val_dataset = CaptioningData(val_data, captions)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=2, collate_fn=val_dataset.collate_fn)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Move model to the appropriate device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Checkpoint directory\n",
        "checkpoint_dir = \"/content/drive/MyDrive/BlipModel/model\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Load from checkpoint if exists\n",
        "start_epoch = 0\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "# Training loop with checkpoint saving\n",
        "model.train()\n",
        "for epoch in range(start_epoch, 50):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = os.path.join(checkpoint_dir, \"final_model\")\n",
        "model.save_pretrained(final_model_path)\n",
        "processor.save_pretrained(final_model_path)\n",
        "\n",
        "# Example to load and use the model\n",
        "loaded_model = BlipForConditionalGeneration.from_pretrained(final_model_path).to(device)\n",
        "loaded_processor = AutoProcessor.from_pretrained(final_model_path)\n",
        "\n",
        "# Generate captions for examples\n",
        "example = dataset[\"test\"][0]\n",
        "image = Image.open(io.BytesIO(example[\"image\"][\"bytes\"])).convert('RGB')\n",
        "inputs = loaded_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "pixel_values = inputs.pixel_values\n",
        "\n",
        "generated_ids = loaded_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "generated_caption = loaded_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(generated_caption)\n",
        "\n",
        "# Visualization of generated captions\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "\n",
        "for i, example in enumerate(dataset[\"test\"][:6]):\n",
        "    image = Image.open(io.BytesIO(example[\"image\"][\"bytes\"])).convert('RGB')\n",
        "    inputs = loaded_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = loaded_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = loaded_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    fig.add_subplot(2, 3, i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Generated caption: {generated_caption}\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bMvU_N_hqq1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}